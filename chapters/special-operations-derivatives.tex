% perhaps taylor series can go here
% also that polynomial rule i made up lols
% let's revisit some calculus as well, how we can apply to interpolation and stuff
% splines?? i want to learn how to do this
% could eventually reach a point we can interpolate using derivatives, would be very cool
% MUST rename -- this is a really generic annoying name
\section{Further Operations on Piecewise Functions}
\subsection{Differentiation}
The differentiability of piecewise functions has been covered frequently in calculus courses, as both practice for differentiation and its applications, as well as for general awareness of piecewise functions (although these are introduced usually in a pre-calculus setting). The nuances of differentiating a piecewise function exceed the number of pages I wish to put in these notes, so I suggest sticking to your typical calculus or analysis book for all of that.

With this being said, we can apply differentiation to interpolated functions (anonymous piecewise functions).

\begin{example}
    Let us find a polynomial function $f(x)$ such that $f(0)=0$, $f(1)=1$ and $f'(0)=0$, $f'(1)=0$.

    Recall the general solution of the APO using a polynomial extraction as per \ref{section:polynomial_general}:
    $$
        f^\star(x) = f(x) + [f(x)]\cdot r(x)
    $$

    In our problem, we note the solution to the first derivative problem is $f(x)=x$; the general solution is therefore:
    $$
        f^\star(x) = x + x(x-1)r(x)
    $$

    Differentiating $f^\star(x)$ we have that:
    $$
        \dv{f^\star}{x} = 1 + (2x-1)r(x) + x(x-1)r'(x)
    $$

    Using the definition of $f'(x)$ and the above equation, we substitute $x=0$ and $x=1$:
    \begin{align*}
        \dv{f^\star}{x}(0) = 1-r(0) &= 0 \implies r(0) =1 \\
        \dv{f^\star}{x}(1) = 1+r(1) &= 0 \implies r(1) =-1
    \end{align*}

    Regard, now, that we have an interpolation problem in $r(x)$:
    $$
        r(x) = \begin{piecewise}
            1 & x = 0 \\
            -1 & x = 1
        \end{piecewise}
    $$

    Note that one could have solved for $r(x)$ directly, making use of the interpolation problem of $f'(x)$, with $r'(x)$ terms (as the term vanishes), and then substituted in the appropriate values accordingly. Furthermore, one solution to this problem is $r(x)=1-2x$ (the corresponding general solution is $r^\star(x)=1-2x+x(x-1)s(x)$).

    Using this, we have a solution given by:
    $$
        f(x) = x + x(x-1)(1-2x)
    $$
\end{example}

\begin{example}
    Let us find a polynomial function which corresponds to the following table (that is, for values of $x$ we have the corresponding values of $f(x)$, and so forth):

    \par{}

    \begin{center}
        \begin{tabular}{c|ccc}
            $x$ & $f(x)$ & $f'(x)$ & $f''(x)$ \\
            \hline
            $1$ & $1$ & $0$ & $1$ \\
            $2$ & $2$ & $0$ & $2$
        \end{tabular}
    \end{center}

    Interpolating $f(x)$, we have the following general solution:
    $$
        f(x) = x + (x-1)(x-2)r_0(x)
    $$

    We can now get the first and second derivative:
    \begin{align*}
        f'(x) &= 1 + (2x-3)r_0(x) + (x-1)(x-2)r'_0(x) \\
        f''(x) &= 2r_0(x) + 2(2x-3)r'_0(x) + (x-1)(x-2)r''_0(x)
    \end{align*}

    Using our first derivative, we obtain the following:
    \begin{align*}
        f'(1) = 1 - r_0(1) &= 0 \implies r_0(1) = 1 \\
        f'(2) = 1 + r_0(2) &= 0 \implies r_0(2) = -1
    \end{align*}

    Subsequently, we have the general solution to $r_0(x)$ (note that we are now repeating the process for $r_0$ instead of $f$):
    $$
        r_0(x) = 3-2x + (x-1)(x-2)r_1(x)
    $$

    Using these values, we can now obtain $r'_0(x)$:
    \begin{align*}
        f''(1) = 2r_0(1) - 2r'_0(1) &= 0 \implies r'_0(1)=\frac{1}{2} \\
        f''(2) = 2r_0(2) + 2r'_0(2) &= 0 \implies r'_0(2)=2
    \end{align*}

    Taking the derivative of $r_0(x)$ we have that:
    $$
        r'_0(x) = -2 + (2x-3)r_1(x) + (x-1)(x-2)r'_1(x)
    $$

    And repeating the process as before for the values of $r_1(x)$:
    \begin{align*}
        r'_0(1) = -2 - r_1(1) &= \frac{1}{2} \implies r_1(1) = -\frac{5}{2} \\
        r'_0(2) = -2 + r_1(2) &= 2 \implies r_1(2) = 4
    \end{align*}

    We therefore have a solution to $r_1$; $r_1(x)=-\frac{5}{2}+\frac{13}{2}(x-1)$. Therefore we have for $r_0$:
    $$
        r_0(x) = 3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}
    $$

    And therefore $f(x)$:
    $$
        f(x) = x + (x-1)(x-2)\paren*{3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}}
    $$
\end{example}

As given by the examples above, the process of interpolating with respect to not only points, but derivatives at those points, is given by iterating through functions until we've satisfied all of our general solutions. This is the process which gives rise to the derivation of the Taylor polynomial (and series).

\begin{theorem}
    \label{theorem:shared_derivatives_theorem}
    All polynomials which share a finite number of points and up to the $n^\text{th}$ derivatives at those points can be written in the form:
    $$
        \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot r(x)
    $$

    Where $[\phi(x)]$ is the decider function, $\mu(x)$ is a solution to the original interpolation problem and $r(x)$ is some arbitrary polynomial.

    \begin{proof}
        We begin by considering all polynomials of the form $\phi^\star(x)=\mu(x)+[\phi(x)]\cdot r(x)$.

        Differentiating $n$ times, we get:
        $$
            \phi^{\star(n)}(x) = \phi^{(n)}(x) + \sum_{m=0}^{n}{\begin{pmatrix} n \\ m \end{pmatrix}r(x)^{(n-m)}\cdot [\phi(x)]^{(m)}}
        $$

        By definition, we require that $\phi^{\star(n)}(x)=\phi^{(n)}(x)$ when each $[\phi(x)]^{(m)}$ vanish, for $m\in\br{0,1,\dots,n-1}$. That is, we want $\phi^{(n)}(x)$ to be a solution to $\phi^{\star(n)}(x)$ for all of our appropriate points (so we satisfy the $n^\text{th}$ derivative requirement when each of the derivatives before the $n^\text{th}$ derivative, of the decider, vanish; almost like induction).

        Using the above, we're left with the problem:
        $$
            [\phi(x)]^{(n)}\cdot r(x)=0
        $$

        The case where $[\phi(x)]^{(n)}$ vanishes yields a differential equation (or set of) which yields the trivial solution, so we'll focus on the other case: where $r(x)$ is a non-zero polynomial which vanishes.

        Using the fact given before, where each decider vanishes, we know that $r(x)$ should at least vanish where the deciders vanish; that is, $r(x)$ can be given by, for some arbitrary polynomial $p(x)$:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]^{(m)}}
        $$

        Furthermore, regard that for all $m\in\br{0,1,\dots,n-1}$ we have that $[\phi(x)]^{(m)}$ vanishes when $[\phi(x)]$ does. That is, for some arbitrary polynomials $t_m(x)$:
        $$
            [\phi(x)]^{(m)}=[\phi(x)]\cdot t_m(x)
        $$

        Therefore:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]\cdot t_{m}(x)} = [\phi(x)]^{n}\cdot p(x)
        $$

        Using the definition of $r(x)$, therefore, we have that:
        $$
            \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot p(x)
        $$
    \end{proof}
\end{theorem}

\begin{theorem}
    \label{theorem:analytic_function_equality}
    As a corollary to \ref{theorem:shared_derivatives_theorem}, suppose we have two analytic functions which share infinitely many derivatives at an arbitrary non-zero number of points. Then these functions must be identical everywhere.

    \begin{proof}
        Let $n\in\mathbb{Z}^+$. We denote the functions described above as $\phi^\star(x)$, and respective `solutions' as $\mu(x)$.

        Since $\phi^\star(x)$ and $\mu(x)$ are analytic functions, we have that $\phi^\star(x)=\mu^\star(x)+\cancelto{0}{O(x^n)}$ and $\mu(x)=M(x)+\cancelto{0}{O(x^n)}$ (as $n\to\infty$). We therefore have that:
        $$
            \varphi^\star(x) = \lim_{n\to\infty}{M(x) + [\phi(x)]^{n+1}\cdot r(x)}
        $$

        Then $r(x)$ is necessarily $0$ since $\varphi^\star(x)$ is continuous. Therefore:
        $$
            \varphi^\star(x) = M(x) \implies \phi^\star(x) = \phi(x)
        $$
    \end{proof}
\end{theorem}

\subsection{Taylor Polynomials}
We have now established two theorems which allow us to express the equality of polynomials, and analytic functions, through shared derivatives. We are therefore ready to establish the idea of a Taylor series: a function which encodes an infinite number of derivatives at a single point.

\begin{theorem}
    \label{theorem:taylor_polynomial}
    Let $\br{a_n}$ be a sequence such that for some analytic function $f$ and $a\in\mathbb{R}$ we have that $f^{(n)}(a)=a_n$. Then $f(x)$ can be given by:
    $$
        f(x) = \sum_{k=0}^{n}{\frac{a_k}{k!}(x-a)^k}+F(x)(x-a)^{n+1}
    $$

    \begin{proof}
        From the above, we have the following system of APOs:
        \begin{align*}
            f(x) &= \begin{piecewise}
                a_0 & x=a \\
                \star & \star
            \end{piecewise} = a_0 + (x-a)F_{0,0}(x) \\
            f'(x) &= \begin{piecewise}
                a_1 & x=a \\
                \star & \star
            \end{piecewise} = a_1 + (x-a)F_{0,1}(x) \\
            \vdots \\
            f^{(n)}(x) &= \begin{piecewise}
                a_n & x=a \\
                \star & \star
            \end{piecewise} = a_n + (x-a)F_{0,n}(x) \\
        \end{align*}

        Explicitly, we write that for $m\geq 0$:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f^{(m)}(x) &= a_m + (x-a)F_{0,m}(x)
        \end{align*}

        From the first equation, we can differentiate $m$ times, and so it can be inductively shown that:
        $$
            f^{(m)}(x) = mF^{(m-1)}_{0,0}(x)+F^{(m)}_{0,0}(x)
        $$

        Letting $x=a$ and equating this with the second of our equations, we have that:
        $$
            F^{(m-1)}_{0,0}(a)=\frac{a_m}{m}
        $$

        We now have a set of new functions (similar to our examples), and so repeating the same technique as before gives us:
        \begin{align*}
            F_{0,0}(x) &= a_1 + (x-a)F_{1,1}(x) \\
            F^{(m-1)}_{0,0} &= \frac{a_m}{m} + (x-a)F_{1,m}(x)
        \end{align*}

        This is nearly identical to the first set of equations, with two noticable differences:
        \begin{itemize}
            \item $m\geq 1$, and so there is one less equation in the system than before; fewer derivatives.
            \item The leading term $\frac{a_m}{m}$ has differed from the original $a_m$ and will be affected by the derivatives (this will, in fact, result in a factorial when this process is repeated).
        \end{itemize}

        From this setup, we set up an induction (to repeat this process) for which $0\leq k<n$:
        \begin{align*}
            F_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x) \\
            F^{(m-k-1)}_{k,k}(x) &= \frac{a_m}{m(m-1)\dots(m-k)}+(x-a)F_{k+1,m}(x)
        \end{align*}

        Since we know this holds for $k=0$, as this case satisfies the above, let us suppose for $0\leq k<n$ we have the above hold for the $k+1$ case. From the first equation, we can inductively prove that:
        $$
            F^{(m-k-1)}_{k,k}(x)=(m-k-1)F^{(m-k-2)}_{k+1,k+1}(x)+(x-a)F^{(m-k-1)}_{k+1,k+1}(x)
        $$

        Equating with the second equation as before, we have that, for $x=a$:
        $$
            F^{(m-k-2)}_{k+1,k+1}(a) = \frac{a_m}{m(m-1)\dots(m-k-1)}
        $$

        This yields the following:
        \begin{align*}
            F^{(m-k-2)}_{k+1,k+1}(x) &= \frac{a_m}{m(m-1)\dots(m-k-1)}+(x-a)F_{k+2,m}(x) \\
            F_{k+1,k+1}(x) &= \frac{a_{k+2}}{(k+2)!}+(x-a)F_{k+2,k+2}(x)
        \end{align*}

        Where the latter equation is derived from the former; $m=k+2$. We now have the following relevant formulas:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x)
        \end{align*}

        Expanding recursively on $F_{0,0}(x)$ gives:
        $$
            f(x) = a_0 + \frac{a_1}{1!}(x-a)+\frac{a_2}{2!}(x-a)^2+\dots+\frac{a_n}{n!}(x-a)^n+F_{n,n}(x)(x-a)^{n+1}
        $$
    \end{proof}

    For a minimal polynomial with these derivatives, one can let $F_{n,n}(x)=0$.
\end{theorem}

% see also: hermite interpolation. is there ANY way we can do this systematically using only piecewise notation no solving????
% **really** really want to do it

\newpage