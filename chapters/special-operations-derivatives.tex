% perhaps taylor series can go here
% also that polynomial rule i made up lols
% let's revisit some calculus as well, how we can apply to interpolation and stuff
% splines?? i want to learn how to do this
% could eventually reach a point we can interpolate using derivatives, would be very cool
% MUST rename -- this is a really generic annoying name
\section{Further Operations on Piecewise Functions}

\subsection{Differentiation}
The differentiability of piecewise functions has been covered frequently in calculus courses, as both practice for differentiation and its applications, as well as for general awareness of piecewise functions (although these are introduced usually in a pre-calculus setting). The nuances of differentiating a piecewise function exceed the number of pages I wish to put in these notes, so I suggest sticking to your typical calculus or analysis book for all of that.

With this being said, we can apply differentiation to interpolated functions (anonymous piecewise functions).

\begin{example}
    Let us find a polynomial function $f(x)$ such that $f(0)=0$, $f(1)=1$ and $f'(0)=0$, $f'(1)=0$.

    Recall the general solution of the APO using a polynomial extraction as per \ref{section:polynomial_general}:
    $$
        f^\star(x) = f(x) + [f(x)]\cdot r(x)
    $$

    In our problem, we note the solution to the first derivative problem is $f(x)=x$; the general solution is therefore:
    $$
        f^\star(x) = x + x(x-1)r(x)
    $$

    Differentiating $f^\star(x)$ we have that:
    $$
        \dv{f^\star}{x} = 1 + (2x-1)r(x) + x(x-1)r'(x)
    $$

    Using the definition of $f'(x)$ and the above equation, we substitute $x=0$ and $x=1$:
    \begin{align*}
        \dv{f^\star}{x}(0) = 1-r(0) &= 0 \implies r(0) =1 \\
        \dv{f^\star}{x}(1) = 1+r(1) &= 0 \implies r(1) =-1
    \end{align*}

    Regard, now, that we have an interpolation problem in $r(x)$:
    $$
        r(x) = \begin{piecewise}
            1 & x = 0 \\
            -1 & x = 1
        \end{piecewise}
    $$

    Note that one could have solved for $r(x)$ directly, making use of the interpolation problem of $f'(x)$, with $r'(x)$ terms (as the term vanishes), and then substituted in the appropriate values accordingly. Furthermore, one solution to this problem is $r(x)=1-2x$ (the corresponding general solution is $r^\star(x)=1-2x+x(x-1)s(x)$).

    Using this, we have a solution given by:
    $$
        f(x) = x + x(x-1)(1-2x)
    $$
\end{example}

\begin{example}
    Let us find a polynomial function which corresponds to the following table (that is, for values of $x$ we have the corresponding values of $f(x)$, and so forth):

    \par{}

    \begin{center}
        \begin{tabular}{c|ccc}
            $x$ & $f(x)$ & $f'(x)$ & $f''(x)$ \\
            \hline
            $1$ & $1$ & $0$ & $1$ \\
            $2$ & $2$ & $0$ & $2$
        \end{tabular}
    \end{center}

    Interpolating $f(x)$, we have the following general solution:
    $$
        f(x) = x + (x-1)(x-2)r_0(x)
    $$

    We can now get the first and second derivative:
    \begin{align*}
        f'(x) &= 1 + (2x-3)r_0(x) + (x-1)(x-2)r'_0(x) \\
        f''(x) &= 2r_0(x) + 2(2x-3)r'_0(x) + (x-1)(x-2)r''_0(x)
    \end{align*}

    Using our first derivative, we obtain the following:
    \begin{align*}
        f'(1) = 1 - r_0(1) &= 0 \implies r_0(1) = 1 \\
        f'(2) = 1 + r_0(2) &= 0 \implies r_0(2) = -1
    \end{align*}

    Subsequently, we have the general solution to $r_0(x)$ (note that we are now repeating the process for $r_0$ instead of $f$):
    $$
        r_0(x) = 3-2x + (x-1)(x-2)r_1(x)
    $$

    Using these values, we can now obtain $r'_0(x)$:
    \begin{align*}
        f''(1) = 2r_0(1) - 2r'_0(1) &= 0 \implies r'_0(1)=\frac{1}{2} \\
        f''(2) = 2r_0(2) + 2r'_0(2) &= 0 \implies r'_0(2)=2
    \end{align*}

    Taking the derivative of $r_0(x)$ we have that:
    $$
        r'_0(x) = -2 + (2x-3)r_1(x) + (x-1)(x-2)r'_1(x)
    $$

    And repeating the process as before for the values of $r_1(x)$:
    \begin{align*}
        r'_0(1) = -2 - r_1(1) &= \frac{1}{2} \implies r_1(1) = -\frac{5}{2} \\
        r'_0(2) = -2 + r_1(2) &= 2 \implies r_1(2) = 4
    \end{align*}

    We therefore have a solution to $r_1$; $r_1(x)=-\frac{5}{2}+\frac{13}{2}(x-1)$. Therefore we have for $r_0$:
    $$
        r_0(x) = 3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}
    $$

    And therefore $f(x)$:
    $$
        f(x) = x + (x-1)(x-2)\paren*{3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}}
    $$
\end{example}

As given by the examples above, the process of interpolating with respect to not only points, but derivatives at those points, is given by iterating through functions until we've satisfied all of our general solutions. This is the process which gives rise to the derivation of the Taylor polynomial (and series).

\begin{theorem}
    \label{theorem:shared_derivatives_theorem}
    All polynomials which share a finite number of points and up to the $n^\text{th}$ derivatives at those points can be written in the form:
    $$
        \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot r(x)
    $$

    Where $[\phi(x)]$ is the decider function, $\mu(x)$ is a solution to the original interpolation problem and $r(x)$ is some arbitrary polynomial.

    \begin{proof}
        We begin by considering all polynomials of the form $\phi^\star(x)=\mu(x)+[\phi(x)]\cdot r(x)$.

        Differentiating $n$ times, we get:
        $$
            \phi^{\star(n)}(x) = \phi^{(n)}(x) + \sum_{m=0}^{n}{\begin{pmatrix} n \\ m \end{pmatrix}r(x)^{(n-m)}\cdot [\phi(x)]^{(m)}}
        $$

        By definition, we require that $\phi^{\star(n)}(x)=\phi^{(n)}(x)$ when each $[\phi(x)]^{(m)}$ vanish, for $m\in\br{0,1,\dots,n-1}$. That is, we want $\phi^{(n)}(x)$ to be a solution to $\phi^{\star(n)}(x)$ for all of our appropriate points (so we satisfy the $n^\text{th}$ derivative requirement when each of the derivatives before the $n^\text{th}$ derivative, of the decider, vanish; almost like induction).

        Using the above, we're left with the problem:
        $$
            [\phi(x)]^{(n)}\cdot r(x)=0
        $$

        The case where $[\phi(x)]^{(n)}$ vanishes yields a differential equation (or set of) which yields the trivial solution, so we'll focus on the other case: where $r(x)$ is a non-zero polynomial which vanishes.

        Using the fact given before, where each decider vanishes, we know that $r(x)$ should at least vanish where the deciders vanish; that is, $r(x)$ can be given by, for some arbitrary polynomial $p(x)$:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]^{(m)}}
        $$

        Furthermore, regard that for all $m\in\br{0,1,\dots,n-1}$ we have that $[\phi(x)]^{(m)}$ vanishes when $[\phi(x)]$ does. That is, for some arbitrary polynomials $t_m(x)$:
        $$
            [\phi(x)]^{(m)}=[\phi(x)]\cdot t_m(x)
        $$

        Therefore:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]\cdot t_{m}(x)} = [\phi(x)]^{n}\cdot p(x)
        $$

        Using the definition of $r(x)$, therefore, we have that:
        $$
            \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot p(x)
        $$
    \end{proof}
\end{theorem}

\begin{theorem}
    \label{theorem:analytic_function_equality}
    As a corollary to \ref{theorem:shared_derivatives_theorem}, suppose we have two analytic functions which share infinitely many derivatives at an arbitrary non-zero number of points. Then these functions must be identical everywhere (where these functions are analytic).

    \begin{proof}
        Let $n\in\mathbb{Z}^+$. We denote the functions described above as $\phi^\star(x)$, and respective `solutions' as $\mu(x)$.

        Since $\phi^\star(x)$ and $\mu(x)$ are analytic functions, we have that $\phi^\star(x)=\mu^\star(x)+\cancelto{0}{O(x^n)}$ and $\mu(x)=M(x)+\cancelto{0}{O(x^n)}$ (as $n\to\infty$). We therefore have that:
        $$
            \varphi^\star(x) = \lim_{n\to\infty}{M(x) + [\phi(x)]^{n+1}\cdot r(x)}
        $$

        Then $r(x)$ is necessarily $0$ since $\varphi^\star(x)$ is continuous. Therefore:
        $$
            \varphi^\star(x) = M(x) \implies \phi^\star(x) = \phi(x)
        $$
    \end{proof}
\end{theorem}

\subsection{Taylor Polynomials}
We have now established two theorems which allow us to express the equality of polynomials, and analytic functions, through shared derivatives. We are therefore ready to establish the idea of a Taylor series: a function which encodes an infinite number of derivatives at a single point.

\begin{theorem}
    \label{theorem:taylor_polynomial}
    Let $\br{a_n}$ be a sequence such that for some analytic function $f$ and $a\in\mathbb{R}$ we have that $f^{(n)}(a)=a_n$. Then $f(x)$ can be given by:
    $$
        f(x) = \sum_{k=0}^{n}{\frac{a_k}{k!}(x-a)^k}+F(x)(x-a)^{n+1}
    $$

    \begin{proof}
        From the above, we have the following system of APOs:
        \begin{align*}
            f(x) &= \begin{piecewise}
                a_0 & x=a \\
                \star & \star
            \end{piecewise} = a_0 + (x-a)F_{0,0}(x) \\
            f'(x) &= \begin{piecewise}
                a_1 & x=a \\
                \star & \star
            \end{piecewise} = a_1 + (x-a)F_{0,1}(x) \\
            \vdots \\
            f^{(n)}(x) &= \begin{piecewise}
                a_n & x=a \\
                \star & \star
            \end{piecewise} = a_n + (x-a)F_{0,n}(x) \\
        \end{align*}

        Explicitly, we write that for $m\geq 0$:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f^{(m)}(x) &= a_m + (x-a)F_{0,m}(x)
        \end{align*}

        From the first equation, we can differentiate $m$ times, and so it can be inductively shown that:
        $$
            f^{(m)}(x) = mF^{(m-1)}_{0,0}(x)+F^{(m)}_{0,0}(x)
        $$

        Letting $x=a$ and equating this with the second of our equations, we have that:
        $$
            F^{(m-1)}_{0,0}(a)=\frac{a_m}{m}
        $$

        We now have a set of new functions (similar to our examples), and so repeating the same technique as before gives us:
        \begin{align*}
            F_{0,0}(x) &= a_1 + (x-a)F_{1,1}(x) \\
            F^{(m-1)}_{0,0} &= \frac{a_m}{m} + (x-a)F_{1,m}(x)
        \end{align*}

        This is nearly identical to the first set of equations, with two noticable differences:
        \begin{itemize}
            \item $m\geq 1$, and so there is one less equation in the system than before; fewer derivatives.
            \item The leading term $\frac{a_m}{m}$ has differed from the original $a_m$ and will be affected by the derivatives (this will, in fact, result in a factorial when this process is repeated).
        \end{itemize}

        From this setup, we set up an induction (to repeat this process) for which $0\leq k<n$:
        \begin{align*}
            F_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x) \\
            F^{(m-k-1)}_{k,k}(x) &= \frac{a_m}{m(m-1)\dots(m-k)}+(x-a)F_{k+1,m}(x)
        \end{align*}

        Since we know this holds for $k=0$, as this case satisfies the above, let us suppose for $0\leq k<n$ we have the above hold for the $k+1$ case. From the first equation, we can inductively prove that:
        $$
            F^{(m-k-1)}_{k,k}(x)=(m-k-1)F^{(m-k-2)}_{k+1,k+1}(x)+(x-a)F^{(m-k-1)}_{k+1,k+1}(x)
        $$

        Equating with the second equation as before, we have that, for $x=a$:
        $$
            F^{(m-k-2)}_{k+1,k+1}(a) = \frac{a_m}{m(m-1)\dots(m-k-1)}
        $$

        This yields the following:
        \begin{align*}
            F^{(m-k-2)}_{k+1,k+1}(x) &= \frac{a_m}{m(m-1)\dots(m-k-1)}+(x-a)F_{k+2,m}(x) \\
            F_{k+1,k+1}(x) &= \frac{a_{k+2}}{(k+2)!}+(x-a)F_{k+2,k+2}(x)
        \end{align*}

        Where the latter equation is derived from the former; $m=k+2$. We now have the following relevant formulas:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x)
        \end{align*}

        Expanding recursively on $F_{0,0}(x)$ gives:
        $$
            f(x) = a_0 + \frac{a_1}{1!}(x-a)+\frac{a_2}{2!}(x-a)^2+\dots+\frac{a_n}{n!}(x-a)^n+F_{n,n}(x)(x-a)^{n+1}
        $$
    \end{proof}

    For a minimal polynomial with these derivatives, one can let $F_{n,n}(x)=0$.
\end{theorem}

If we extend \ref{theorem:taylor_polynomial} to an infinite number of derivatives, we get a Taylor series. However, we don't have a theorem here which states that knowing an infinite number of derivatives gives us, at least locally, an infinite number of points (equal to a function). This is the property of being real analytic (locally or globally; though actually this definition extends to power series in general). This relates to Taylor's theorem. Furthermore, given the way we've motivated and derived the formula, we nicely move into Borel's theorem too (both of which won't actually be discussed here, but are nice rabbit holes to go down).

% see also: hermite interpolation. is there ANY way we can do this systematically using only piecewise notation no solving????
% **really** really want to do it
% yessss we totally can, extended dual numbers! the epsilons :D

\subsection{Dual numbers (and extensions)}
The dual numbers are a number system in which we have an element, $\varepsilon$, such that $\varepsilon^2=0$. They extend the real numbers in a way we have a real component and a dual number component;
$$
w=a+b\varepsilon
$$

What makes this system very interesting is that we can extend analytic functions to the dual numbers:
$$
f(w)=f(a+b\varepsilon)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(a)}{n!}(b\varepsilon)^n}=f(a)+f'(a)b\varepsilon
$$

That is, we take the Taylor series of $f$ centred at $a$, and evaluate the function at $a+b\varepsilon$. The reason for this being so interesting is that we now have a direct mapping between dual numbers and a function's derivatives.

Circling back in on the idea of interpolation with derivatives, what we'll use is the following fact, in particular:
$$
    f(a)=a_0\land f'(a)=a_1\leftrightarrow f(0)=a_0\land f(a+\varepsilon)=a_0+a_1\varepsilon
$$

This allows us to interpolate functions with values and first derivatives. We do so by taking in function values for $f$, generating a corresponding dual number-valued function $F$ and then extracting our whole solution for $f$.

This above idea is actually closely related to multivariate interpolation: for some values of $x, f'$, we want to find $f$. In some sense, we can think of $f'$ as another argument or dimension, although we don't explicitly want to use it.

\begin{example}
    We wish to find a polynomial function and an exponential function, $f$, $g$ such that $f(0)=g(0)=1$ and $f'(0)=g'(0)=1$.

    Note that $f(0)=0\land f'(0)=1\iff f(\varepsilon)=1+\varepsilon$. Then, beginning with the polynomial, we have:
    \begin{align*}
        F(w) &= \begin{piecewise}
            1 & w=0 \\
            1+\varepsilon & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= 1+w\begin{piecewise}
            \frac{\varepsilon}{\varepsilon} & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= 1+w
    \end{align*}

    Substituting in the appropriate values, we see that is correct. Furthermore, if we take $F(a+b\varepsilon)$, we get the real part is given by $1+a$. Therefore, $f(x)=1+x$.

    We now perform similar to find an exponential function:
    \begin{align*}
        G(w) &= \begin{piecewise}
            1 & w=0 \\
            1+\varepsilon & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= \paren*{\begin{piecewise}
            (1+\varepsilon)^\frac{1}{\varepsilon} & w=\varepsilon \\
            \star & \star
        \end{piecewise}}^w \\
    \end{align*}

    Note that $(1+\varepsilon)^\frac{1}{\varepsilon}\defeq e$, since we might (abusively) write it as $\exp(\frac{1}{\varepsilon}\ln(1+\varepsilon))$, and recursively evaluate. This gives us:
    $$
        G(w)=e^w
    $$

    Since $g(a)=\Re{G(a+b\varepsilon)}$ we have $g(x)=e^x$, after taking the real part.
\end{example}

As we saw in this example, we actually divided by $\varepsilon$. We can't normally do this. But since we're simply trying to assign a value to various coefficients, it's totally fine, if not a little bit abusive of notation. Though, by argument, we should verify that our answer is correct by substituting in the appropriate values; interestingly, the non-indeterminate values we assign to problematic expressions like $\frac{\varepsilon}{\varepsilon}$ end up being correct for our uses. This becomes a significantly more important idea in the extension of the dual numbers.

On another note, we needn't perform the final substitution $a+b\varepsilon$; we may take the real part directly. This is to do with how $\varepsilon^2$ vanishes anyway, and the we disregard the imaginary part at the end regardless (though if we didn't, we'd observe the derivative and vanishing points).

\subsubsection{Dual number extension}
When we define $\varepsilon^2=0$ but $\varepsilon\neq 0$, we're actually defining a nilpotent element and an indeterminate. That is, $\varepsilon$ doesn't have any specific value per se, but there exists a power $n$ for which $\varepsilon^n=0$. There exists a matrix representation, or several, rather, for $\varepsilon$:
$$
    \varepsilon=\begin{pmatrix}
        0 & 1 \\
        0 & 0
    \end{pmatrix}
$$

And so, in some sense, we can give $\varepsilon$, and therefore dual numbers, a value. $w=a+b\varepsilon$ ends up being represented by:
$$
    w=\begin{pmatrix}
        a & b \\
        0 & a
    \end{pmatrix}
$$
This can be given by multiplying a scalar, $a$, by the $2\times 2$ identity matrix. Hooray notation.

But now, a fairly straightforward question emerges: Can we get further derivatives using this idea? Can we embed or encode such a thing by extending the dual numbers? Should such a thing exist? Well, I can't answer the last question, but let us define the following:

\begin{theorem}
    For some $N\in\mathbb{N}$ and for all $n\in\br{1,2,\dots,N}$, we define some extension of the dual numbers using the elements:
    $$
        \varepsilon_{n}^{n+1}=0
    $$

    Therefore, we can represent all elements in this number system using the following:
    $$
        \sum_{n=1}^{N}{\sum_{m=1}^{n-1}{a_{n,m}\cdot\varepsilon_{n}^{m}}}
    $$

    This is because we haven't defined $\varepsilon_{n}^{n}$ and so on to be $0$.
\end{theorem}

\begin{theorem}
    In our $N$-extended dual number system, for all $i,j\in\br{1,2,\dots,N}$ and $i\neq j$, we have that:
    $$
        \varepsilon_i\cdot\varepsilon_j=\varepsilon_j\cdot\varepsilon_i=0
    $$

    \begin{proof}
        Suppose we have that $A=\varepsilon_i\varepsilon_j$ for some $j\in\br{1,2,\dots,N}$.

        Then $\varepsilon_i^i\cdot A=\varepsilon_i^i\cdot\varepsilon_i\cdot\varepsilon_j=\varepsilon_i^{i+1}\cdot\varepsilon_j$.

        The RHS is $0$ by definition, and so we have that $\varepsilon_i^i\cdot A=0$. Since $\varepsilon_i^i$ is a non-zero element, we must have $A=0$. This argument can be identically repeated for $\varepsilon_j\cdot\varepsilon_i$.
    \end{proof}

    In other words, we don't care about commutativity under multiplication.
\end{theorem}

\begin{theorem}
    A `simple' analytic extension of an analytic function to the $N$-extended dual numbers can be given by:
    $$
        f(a+b\varepsilon_m)=\sum_{n=0}^{m}{\frac{f^{(n)}(a)}{n!}\paren{b\varepsilon_m}^n}
    $$

    These are not the only extensions which are relevant, but the extensions with all possible terms are just messy and so as-needed is perhaps better for those individual cases.
    \begin{proof}
        The analytic function $f$ can be represented by its Taylor series centred around $a$:
        $$
            f(x) = \sum_{n=0}^{\infty}{\frac{f^{(n)(a)}}{n!}(x-a)^n}
        $$

        Suppose we evaluate $f(x)$ at $x=a+b\varepsilon_m$, we get that:
        $$
            f(a+b\varepsilon_m) = \sum_{n=0}^{\infty}{\frac{f^{(n)(a)}}{n!}(b\varepsilon_m)^n}
        $$

        Since all terms beyond $n=m$ vanish by $\varepsilon_m^{m+1}=0$, we rewrite the sum as
        $$
            f(a+b\varepsilon_m) = \sum_{n=0}^{m}{\frac{f^{(n)(a)}}{n!}(b\varepsilon_m)^n}
        $$
    \end{proof}
\end{theorem}

In order to encode $f(a),f'(a),\dots,f^{(n)}(a)$ as a function, we require a definition for all of $f(a),f(a+b\varepsilon_1),\dots,f(a+b\varepsilon_n)$. Intuitively, this is because the value of the function at each extended point provides several pieces of data (i.e. up to whatever derivative), and so we need to recurse down in order to retrieve each derivative. More or less.

Furthermore, we apply the same technique as in the regular dual numbers in order to interpolate our function. We choose to encode it as:
$$
    f(x_1,x_2,\dots,x_n)=\Re{F(x_1+x_2\varepsilon_1+\dots+x_n\varepsilon_{n-1})}
$$

This is for simplicity more than anything.

\newpage