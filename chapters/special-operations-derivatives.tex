% perhaps taylor series can go here
% also that polynomial rule i made up lols
% let's revisit some calculus as well, how we can apply to interpolation and stuff
% splines?? i want to learn how to do this
% could eventually reach a point we can interpolate using derivatives, would be very cool
% MUST rename -- this is a really generic annoying name
\chapter{Further Operations on Piecewise Functions}

\section{Differentiation}
The differentiability of piecewise functions has been covered frequently in calculus courses, as both practice for differentiation and its applications, as well as for general awareness of piecewise functions (although these are introduced usually in a pre-calculus setting). The nuances of differentiating a piecewise function exceed the number of pages I wish to put in these notes, so I suggest sticking to your typical calculus or analysis book for all of that.

With this being said, we can apply differentiation to interpolated functions (anonymous piecewise functions).

\begin{example}
    Let us find a polynomial function $f(x)$ such that $f(0)=0$, $f(1)=1$ and $f'(0)=0$, $f'(1)=0$.

    Recall the general solution of the APO using a polynomial extraction as per \autoref{section:polynomial_general}:
    $$
        f^\star(x) = f(x) + [f(x)]\cdot r(x)
    $$

    In our problem, we note the solution to the first derivative problem is $f(x)=x$; the general solution is therefore:
    $$
        f^\star(x) = x + x(x-1)r(x)
    $$

    Differentiating $f^\star(x)$ we have that:
    $$
        \dv{f^\star}{x} = 1 + (2x-1)r(x) + x(x-1)r'(x)
    $$

    Using the definition of $f'(x)$ and the above equation, we substitute $x=0$ and $x=1$:
    \begin{align*}
        \dv{f^\star}{x}(0) = 1-r(0) &= 0 \implies r(0) =1 \\
        \dv{f^\star}{x}(1) = 1+r(1) &= 0 \implies r(1) =-1
    \end{align*}

    Regard, now, that we have an interpolation problem in $r(x)$:
    $$
        r(x) = \begin{piecewise}
            1 & x = 0 \\
            -1 & x = 1
        \end{piecewise}
    $$

    Note that one could have solved for $r(x)$ directly, making use of the interpolation problem of $f'(x)$, with $r'(x)$ terms (as the term vanishes), and then substituted in the appropriate values accordingly. Furthermore, one solution to this problem is $r(x)=1-2x$ (the corresponding general solution is $r^\star(x)=1-2x+x(x-1)s(x)$).

    Using this, we have a solution given by:
    $$
        f(x) = x + x(x-1)(1-2x)
    $$
\end{example}

\begin{example}
    Let us find a polynomial function which corresponds to the following table (that is, for values of $x$ we have the corresponding values of $f(x)$, and so forth):

    \par{}

    \begin{center}
        \begin{tabular}{c|ccc}
            $x$ & $f(x)$ & $f'(x)$ & $f''(x)$ \\
            \hline
            $1$ & $1$ & $0$ & $1$ \\
            $2$ & $2$ & $0$ & $2$
        \end{tabular}
    \end{center}

    Interpolating $f(x)$, we have the following general solution:
    $$
        f(x) = x + (x-1)(x-2)r_0(x)
    $$

    We can now get the first and second derivative:
    \begin{align*}
        f'(x) &= 1 + (2x-3)r_0(x) + (x-1)(x-2)r'_0(x) \\
        f''(x) &= 2r_0(x) + 2(2x-3)r'_0(x) + (x-1)(x-2)r''_0(x)
    \end{align*}

    Using our first derivative, we obtain the following:
    \begin{align*}
        f'(1) = 1 - r_0(1) &= 0 \implies r_0(1) = 1 \\
        f'(2) = 1 + r_0(2) &= 0 \implies r_0(2) = -1
    \end{align*}

    Subsequently, we have the general solution to $r_0(x)$ (note that we are now repeating the process for $r_0$ instead of $f$):
    $$
        r_0(x) = 3-2x + (x-1)(x-2)r_1(x)
    $$

    Using these values, we can now obtain $r'_0(x)$:
    \begin{align*}
        f''(1) = 2r_0(1) - 2r'_0(1) &= 0 \implies r'_0(1)=\frac{1}{2} \\
        f''(2) = 2r_0(2) + 2r'_0(2) &= 0 \implies r'_0(2)=2
    \end{align*}

    Taking the derivative of $r_0(x)$ we have that:
    $$
        r'_0(x) = -2 + (2x-3)r_1(x) + (x-1)(x-2)r'_1(x)
    $$

    And repeating the process as before for the values of $r_1(x)$:
    \begin{align*}
        r'_0(1) = -2 - r_1(1) &= \frac{1}{2} \implies r_1(1) = -\frac{5}{2} \\
        r'_0(2) = -2 + r_1(2) &= 2 \implies r_1(2) = 4
    \end{align*}

    We therefore have a solution to $r_1$; $r_1(x)=-\frac{5}{2}+\frac{13}{2}(x-1)$. Therefore we have for $r_0$:
    $$
        r_0(x) = 3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}
    $$

    And therefore $f(x)$:
    $$
        f(x) = x + (x-1)(x-2)\paren*{3-2x + (x-1)(x-2)\paren*{-\frac{5}{2}+\frac{13}{2}(x-1)}}
    $$
\end{example}

As given by the examples above, the process of interpolating with respect to not only points, but derivatives at those points, is given by iterating through functions until we've satisfied all of our general solutions. This is the process which gives rise to the derivation of the Taylor polynomial (and series).

\begin{theorem}
    \label{theorem:shared_derivatives_theorem}
    All polynomials which share a finite number of points and up to the $n^\text{th}$ derivatives at those points can be written in the form:
    $$
        \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot r(x)
    $$

    Where $[\phi(x)]$ is the decider function, $\mu(x)$ is a solution to the original interpolation problem and $r(x)$ is some arbitrary polynomial.

    \begin{proof}
        We begin by considering all polynomials of the form $\phi^\star(x)=\mu(x)+[\phi(x)]\cdot r(x)$.

        Differentiating $n$ times, we get:
        $$
            \phi^{\star(n)}(x) = \phi^{(n)}(x) + \sum_{m=0}^{n}{\begin{pmatrix} n \\ m \end{pmatrix}r(x)^{(n-m)}\cdot [\phi(x)]^{(m)}}
        $$

        By definition, we require that $\phi^{\star(n)}(x)=\phi^{(n)}(x)$ when each $[\phi(x)]^{(m)}$ vanish, for $m\in\br{0,1,\dots,n-1}$. That is, we want $\phi^{(n)}(x)$ to be a solution to $\phi^{\star(n)}(x)$ for all of our appropriate points (so we satisfy the $n^\text{th}$ derivative requirement when each of the derivatives before the $n^\text{th}$ derivative, of the decider, vanish; almost like induction).

        Using the above, we're left with the problem:
        $$
            [\phi(x)]^{(n)}\cdot r(x)=0
        $$

        The case where $[\phi(x)]^{(n)}$ vanishes yields a differential equation (or set of) which yields the trivial solution, so we'll focus on the other case: where $r(x)$ is a non-zero polynomial which vanishes.

        Using the fact given before, where each decider vanishes, we know that $r(x)$ should at least vanish where the deciders vanish; that is, $r(x)$ can be given by, for some arbitrary polynomial $p(x)$:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]^{(m)}}
        $$

        Furthermore, regard that for all $m\in\br{0,1,\dots,n-1}$ we have that $[\phi(x)]^{(m)}$ vanishes when $[\phi(x)]$ does. That is, for some arbitrary polynomials $t_m(x)$:
        $$
            [\phi(x)]^{(m)}=[\phi(x)]\cdot t_m(x)
        $$

        Therefore:
        $$
            r(x) = p(x)\prod_{m=0}^{n-1}{[\phi(x)]\cdot t_{m}(x)} = [\phi(x)]^{n}\cdot p(x)
        $$

        Using the definition of $r(x)$, therefore, we have that:
        $$
            \phi^\star(x) = \mu(x) + [\phi(x)]^{n+1}\cdot p(x)
        $$
    \end{proof}
\end{theorem}

\begin{theorem}
    \label{theorem:analytic_function_equality}
    As a corollary to \autoref{theorem:shared_derivatives_theorem}, suppose we have two analytic functions which share infinitely many derivatives at an arbitrary non-zero number of points. Then these functions must be identical everywhere (where these functions are analytic).

    \begin{proof}
        Let $n\in\mathbb{Z}^+$. We denote the functions described above as $\phi^\star(x)$, and respective `solutions' as $\mu(x)$.

        Since $\phi^\star(x)$ and $\mu(x)$ are analytic functions, we have that $\phi^\star(x)=\mu^\star(x)+\cancelto{0}{O(x^n)}$ and $\mu(x)=M(x)+\cancelto{0}{O(x^n)}$ (as $n\to\infty$). We therefore have that:
        $$
            \varphi^\star(x) = \lim_{n\to\infty}{M(x) + [\phi(x)]^{n+1}\cdot r(x)}
        $$

        Then $r(x)$ is necessarily $0$ since $\varphi^\star(x)$ is continuous. Therefore:
        $$
            \varphi^\star(x) = M(x) \implies \phi^\star(x) = \phi(x)
        $$
    \end{proof}
\end{theorem}

\section{Taylor polynomials}
We have now established two theorems which allow us to express the equality of polynomials, and analytic functions, through shared derivatives. We are therefore ready to establish the idea of a Taylor series: a function which encodes an infinite number of derivatives at a single point.

\begin{theorem}
    \label{theorem:taylor_polynomial}
    Let $\br{a_n}$ be a sequence such that for some analytic function $f$ and $a\in\mathbb{R}$ we have that $f^{(n)}(a)=a_n$. Then $f(x)$ can be given by:
    $$
        f(x) = \sum_{k=0}^{n}{\frac{a_k}{k!}(x-a)^k}+F(x)(x-a)^{n+1}
    $$

    \begin{proof}
        From the above, we have the following system of APOs:
        \begin{align*}
            f(x) &= \begin{piecewise}
                a_0 & x=a \\
                \star & \star
            \end{piecewise} = a_0 + (x-a)F_{0,0}(x) \\
            f'(x) &= \begin{piecewise}
                a_1 & x=a \\
                \star & \star
            \end{piecewise} = a_1 + (x-a)F_{0,1}(x) \\
            \vdots \\
            f^{(n)}(x) &= \begin{piecewise}
                a_n & x=a \\
                \star & \star
            \end{piecewise} = a_n + (x-a)F_{0,n}(x) \\
        \end{align*}

        Explicitly, we write that for $m\geq 0$:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f^{(m)}(x) &= a_m + (x-a)F_{0,m}(x)
        \end{align*}

        From the first equation, we can differentiate $m$ times, and so it can be inductively shown that:
        $$
            f^{(m)}(x) = mF^{(m-1)}_{0,0}(x)+F^{(m)}_{0,0}(x)
        $$

        Letting $x=a$ and equating this with the second of our equations, we have that:
        $$
            F^{(m-1)}_{0,0}(a)=\frac{a_m}{m}
        $$

        We now have a set of new functions (similar to our examples), and so repeating the same technique as before gives us:
        \begin{align*}
            F_{0,0}(x) &= a_1 + (x-a)F_{1,1}(x) \\
            F^{(m-1)}_{0,0} &= \frac{a_m}{m} + (x-a)F_{1,m}(x)
        \end{align*}

        This is nearly identical to the first set of equations, with two noticable differences:
        \begin{itemize}
            \item $m\geq 1$, and so there is one less equation in the system than before; fewer derivatives.
            \item The leading term $\frac{a_m}{m}$ has differed from the original $a_m$ and will be affected by the derivatives (this will, in fact, result in a factorial when this process is repeated).
        \end{itemize}

        From this setup, we set up an induction (to repeat this process) for which $0\leq k<n$:
        \begin{align*}
            F_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x) \\
            F^{(m-k-1)}_{k,k}(x) &= \frac{a_m}{m(m-1)\dots(m-k)}+(x-a)F_{k+1,m}(x)
        \end{align*}

        Since we know this holds for $k=0$, as this case satisfies the above, let us suppose for $0\leq k<n$ we have the above hold for the $k+1$ case. From the first equation, we can inductively prove that:
        $$
            F^{(m-k-1)}_{k,k}(x)=(m-k-1)F^{(m-k-2)}_{k+1,k+1}(x)+(x-a)F^{(m-k-1)}_{k+1,k+1}(x)
        $$

        Equating with the second equation as before, we have that, for $x=a$:
        $$
            F^{(m-k-2)}_{k+1,k+1}(a) = \frac{a_m}{m(m-1)\dots(m-k-1)}
        $$

        This yields the following:
        \begin{align*}
            F^{(m-k-2)}_{k+1,k+1}(x) &= \frac{a_m}{m(m-1)\dots(m-k-1)}+(x-a)F_{k+2,m}(x) \\
            F_{k+1,k+1}(x) &= \frac{a_{k+2}}{(k+2)!}+(x-a)F_{k+2,k+2}(x)
        \end{align*}

        Where the latter equation is derived from the former; $m=k+2$. We now have the following relevant formulas:
        \begin{align*}
            f(x) &= a_0 + (x-a)F_{0,0}(x) \\
            f_{k,k}(x) &= \frac{a_{k+1}}{(k+1)!}+(x-a)F_{k+1,k+1}(x)
        \end{align*}

        Expanding recursively on $F_{0,0}(x)$ gives:
        $$
            f(x) = a_0 + \frac{a_1}{1!}(x-a)+\frac{a_2}{2!}(x-a)^2+\dots+\frac{a_n}{n!}(x-a)^n+F_{n,n}(x)(x-a)^{n+1}
        $$
    \end{proof}

    For a minimal polynomial with these derivatives, one can let $F_{n,n}(x)=0$.
\end{theorem}

If we extend \autoref{theorem:taylor_polynomial} to an infinite number of derivatives, we get a Taylor series. However, we don't have a theorem here which states that knowing an infinite number of derivatives gives us, at least locally, an infinite number of points (equal to a function). This is the property of being real analytic (locally or globally; though actually this definition extends to power series in general). This relates to Taylor's theorem. Furthermore, given the way we've motivated and derived the formula, we nicely move into Borel's theorem too (both of which won't actually be discussed here, but are nice rabbit holes to go down).

% see also: hermite interpolation. is there ANY way we can do this systematically using only piecewise notation no solving????
% **really** really want to do it
% yessss we totally can, extended dual numbers! the epsilons :D

\section{Dual numbers (and extensions)}
\label{section:dual_numbers}
The dual numbers are a number system in which we have an element, $\varepsilon$, such that $\varepsilon^2=0$. They extend the real numbers in such a way we have a real component and a dual number component;
$$
w=a+b\varepsilon
$$

What makes this system very interesting is that we can extend analytic functions to the dual numbers:
$$
f(w)=f(a+b\varepsilon)=\sum_{n=0}^{\infty}{\frac{f^{(n)}(a)}{n!}(b\varepsilon)^n}=f(a)+f'(a)b\varepsilon
$$

That is, we take the Taylor series of $f$ centred at $a$, and evaluate the function at $a+b\varepsilon$. The reason for this being so interesting is that we now have a direct mapping between dual numbers and a function's derivatives.

Circling back in on the idea of interpolation with derivatives, what we'll use is the following fact, in particular:
$$
    f(a)=a_0\land f'(a)=a_1\leftrightarrow f(0)=a_0\land f(a+\varepsilon)=a_0+a_1\varepsilon
$$

This allows us to interpolate functions with values and first derivatives. We do so by taking in function values for $f$, generating a corresponding dual number-valued function $F$ and then extracting our whole solution for $f$.

This above idea is actually closely related to multivariate interpolation: for some values of $x, f'$, we want to find $f$. In some sense, we can think of $f'$ as another argument or dimension, although we don't explicitly want to use it.

\begin{example}
    We wish to find a polynomial function and an exponential function, $f$, $g$ such that $f(0)=g(0)=1$ and $f'(0)=g'(0)=1$.

    Note that $f(0)=0\land f'(0)=1\iff f(\varepsilon)=1+\varepsilon$. Then, beginning with the polynomial, we have:
    \begin{align*}
        F(w) &= \begin{piecewise}
            1 & w=0 \\
            1+\varepsilon & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= 1+w\begin{piecewise}
            \frac{\varepsilon}{\varepsilon} & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= 1+w
    \end{align*}

    Substituting in the appropriate values, we see that is correct. Furthermore, if we take $F(a+b\varepsilon)$, we get the real part is given by $1+a$. Therefore, $f(x)=1+x$.

    We now perform similar to find an exponential function:
    \begin{align*}
        G(w) &= \begin{piecewise}
            1 & w=0 \\
            1+\varepsilon & w=\varepsilon \\
            \star & \star
        \end{piecewise} \\
        &= \paren*{\begin{piecewise}
            (1+\varepsilon)^\frac{1}{\varepsilon} & w=\varepsilon \\
            \star & \star
        \end{piecewise}}^w \\
    \end{align*}

    Note that $(1+\varepsilon)^\frac{1}{\varepsilon}\defeq e$, since we might (abusively) write it as $\exp(\frac{1}{\varepsilon}\ln(1+\varepsilon))$, and recursively evaluate. This gives us:
    $$
        G(w)=e^w
    $$

    Since $g(a)=\Re{G(a+b\varepsilon)}$ we have $g(x)=e^x$, after taking the real part.
\end{example}

As we saw in this example, we actually divided by $\varepsilon$. We can't normally do this. But since we're simply trying to assign a value to various coefficients, it's totally fine, if not a little bit abusive of notation. Though, by argument, we should verify that our answer is correct by substituting in the appropriate values; interestingly, the non-indeterminate values we assign to problematic expressions like $\frac{\varepsilon}{\varepsilon}$ end up being correct for our uses. This becomes a significantly more important idea in the extension of the dual numbers.

On another note, we needn't perform the final substitution $a+b\varepsilon$; we may take the real part directly. This is to do with how $\varepsilon^2$ vanishes anyway, and the we disregard the imaginary part at the end regardless (though if we didn't, we'd observe the derivative and vanishing points).

\subsection{Dual number extension}
When we define $\varepsilon^2=0$ but $\varepsilon\neq 0$, we're actually defining a nilpotent element and an indeterminate. That is, $\varepsilon$ doesn't have any specific value per se, but there exists a power $n$ for which $\varepsilon^n=0$. There exists a matrix representation, or several, rather, for $\varepsilon$:
$$
    \varepsilon=\begin{pmatrix}
        0 & 1 \\
        0 & 0
    \end{pmatrix}
$$

And so, in some sense, we can give $\varepsilon$, and therefore dual numbers, a value. $w=a+b\varepsilon$ ends up being represented by:
$$
    w=\begin{pmatrix}
        a & b \\
        0 & a
    \end{pmatrix}
$$
This can be given by multiplying a scalar, $a$, by the $2\times 2$ identity matrix. Hooray notation.

But now, a fairly straightforward question emerges: Can we get further derivatives using this idea? Can we embed or encode such a thing by extending the dual numbers? Should such a thing exist? Well, I can't answer the last question, but let us define the following:

\begin{theorem}
    For some $N\in\mathbb{N}$ and for all $n\in\br{1,2,\dots,N}$, we define some extension of the dual numbers using the elements:
    $$
        \varepsilon_{n}^{n+1}=0
    $$

    Therefore, we can represent all elements in this number system using the following (where $\Re{w}=a$):
    $$
        w=a+\sum_{n=1}^{N}{\sum_{m=1}^{n}{a_{n,m}\cdot\varepsilon_{n}^{m}}}=a+\sum_{1\leq m\leq n\leq N}{a_{n,m}\cdot\varepsilon_{n}^{m}}
    $$
\end{theorem}

\begin{theorem}
    In our $N$-extended dual number system, for all $i,j\in\br{1,2,\dots,N}$ and $i\neq j$, we define the following property:
    $$
        \varepsilon_i\cdot\varepsilon_j=\varepsilon_j\cdot\varepsilon_i=0
    $$

    We choose this out of simplicity. If we, however, decided to define our $\varepsilon$s based on their matrix representations, it is entirely possible this property would not hold and that $\varepsilon_i\cdot \varepsilon_j\not\in\br{\varepsilon_1,\varepsilon_2,\dots,\varepsilon_n}$.
\end{theorem}

\begin{theorem}
    The degree of an $N$-extended dual number is given by the following:
    $$
        \deg{w}=\min\br{k\in\mathbb{Z}^+\mid (w-\Re{w})^{k+1}=0}
    $$

    Where $\Re{w}$ is the real part of $w$. In essence, we describe the degree of $w$ to be the smallest such integer so that the non real part of $w$, raised to $k+1$, is 0. Therefore:
    $$
        \deg{\varepsilon_k}=k
    $$

    As a corollary to the above, the degree of $\varepsilon_{a}^{b}$ for $b\leq a$ is given by:
    $$
        \deg{\varepsilon_{a}^{b}}=\floor*{\frac{a}{b}}
    $$

    It's worth noting that the degree of a real number diverges to positive infinity except for $0$; there is no such positive integer such that a non-zero \textit{real number} raised to that integer will be $0$.
\end{theorem}

\begin{theorem}
    Two $N$-extended dual numbers of the following form:
    \begin{align*}
        u=a+\sum_{n=1}^{N}{a_n\cdot\varepsilon_{n}} \\
        w=b+\sum_{n=1}^{N}{b_n\cdot\varepsilon_{n}}
    \end{align*}

    are equal if and only if $a=b$ and $a_{n,m}=b_{n,m}$ for all appropriate $n,m$.

    \begin{proof}
        We begin by noticing that the substitution $b=a$ and $b_{n,m}=a_{n,m}$ for all $n,m$ gives:
        $$
            w=a+\sum_{n=1}^{N}{a_n\cdot\varepsilon_{n}}=u
        $$

        On the other hand, suppose that we have $u=w$. Then:
        $$
            a-b=\sum_{n=1}^{N}{(b_n-a_n)\cdot\varepsilon_{n}}
        $$

        Raising both sides to $N+1$ gives $(a-b)^{N+1}=0\implies a=b$.

        We then repeat this process with the next highest degree $\varepsilon$; extracting the case $n=N$ to the left hand side gives and raising both sides to $N$:
        $$
            ((a_N-b_N)\varepsilon_N)^{N}=0
        $$

        This is only possible if $a_N=b_N$, since $\varepsilon_N^N\neq 0$. Recursing on each $n$ following, we find that:
        $$
            a_1=b_1,\ a_2=b_2,\ \dots,\ a_N=b_N
        $$
    \end{proof}
\end{theorem}

\begin{theorem}
    No two powers of $N$-extended dual numbers are equal under the addition property of exponentials. That is, that:
    $$
        \varepsilon_{a}^{b}\neq \varepsilon_{\floor{\frac{a}{b}}}
    $$

    \begin{proof}
        Assume by contradiction that for all $a,b\in\br{1,2,\dots,n}$ and $a\leq b$ that the above does not hold. Furthermore, assume that $\varepsilon_m^{a+b}=\varepsilon_m^a\varepsilon_m^b$ for $a+b\leq m$.

        It then stands to reason that for $\varepsilon_{a}^{a+1}=0\iff\varepsilon_a\cdot\varepsilon_a^a=0$. Since $\varepsilon_a^a=\varepsilon_1$, we get that $\varepsilon_a\varepsilon_1=0$.

        We then repeat this process for $\varepsilon_a^a=\varepsilon_1\iff\varepsilon_a\varepsilon_a^{a-1}=\varepsilon_1$. But regard that $\varepsilon_a^{a-1}=\varepsilon_1$ also, and therefore $\varepsilon_a\varepsilon_1=\varepsilon_1$.

        From the above two results, we see that $\varepsilon_1=0$ which is a contradiction.
    \end{proof}
\end{theorem}

\begin{theorem}
    A `simple' analytic extension of an analytic function to the $N$-extended dual numbers can be given by:
    $$
        f(a+b\varepsilon_m)=\sum_{n=0}^{m}{\frac{f^{(n)}(a)}{n!}\paren{b\varepsilon_m}^n}
    $$

    These are not the only extensions which are relevant, but the extensions with all possible terms are just messy and so as-needed is perhaps better for those individual cases.
    \begin{proof}
        The analytic function $f$ can be represented by its Taylor series centred around $a$:
        $$
            f(x) = \sum_{n=0}^{\infty}{\frac{f^{(n)(a)}}{n!}(x-a)^n}
        $$

        Suppose we evaluate $f(x)$ at $x=a+b\varepsilon_m$, we get that:
        $$
            f(a+b\varepsilon_m) = \sum_{n=0}^{\infty}{\frac{f^{(n)(a)}}{n!}(b\varepsilon_m)^n}
        $$

        Since all terms beyond $n=m$ vanish by $\varepsilon_m^{m+1}=0$, we rewrite the sum as
        $$
            f(a+b\varepsilon_m) = \sum_{n=0}^{m}{\frac{f^{(n)(a)}}{n!}(b\varepsilon_m)^n}
        $$
    \end{proof}
\end{theorem}

In order to encode $f(a),f'(a),\dots,f^{(n)}(a)$ as a function, we require a definition for all of $f(a),f(a+b\varepsilon_1),\dots,f(a+b\varepsilon_n)$. Intuitively, this is because the value of the function at each extended point provides several pieces of data (i.e. up to whatever derivative), and so we need to recurse down in order to retrieve each derivative. More or less.

Furthermore, we apply the same technique as in the regular dual numbers in order to interpolate our function. We choose to encode it as:
$$
    f(x)=\Re{F(w)},\quad w=x+x_1\varepsilon_1+x_2\varepsilon_2+\dots+x_n\varepsilon_n
$$

This is for simplicity more than anything.

\begin{example}
    We wish to find a polynomial $f$ such that $f(0)=1$, $f'(0)=2$ and $f''(0)=3$.

    From $f(0)=1$ and $f'(0)=2$, we have that $F(\varepsilon_1)=1+2\varepsilon_1$. Likewise, from $f(0)=1$, $f'(0)=2$ and $f''(0)=3$, we have that $F(\varepsilon_2)=1+2\varepsilon_2+\frac{3}{2}\varepsilon_2^2$.

    We construct our APO and simplify accordingly:
    \begin{align*}
        F(w) &= \begin{piecewise}
            1 & w=0 \\
            1+2\varepsilon_1 & w=\varepsilon_1 \\
            1+2\varepsilon_2+\frac{3}{2}\varepsilon_2^2 & w=\varepsilon_2 \\
            \star & \star
        \end{piecewise} \\
        &= 1+w\begin{piecewise}
            \frac{2\varepsilon_1}{\varepsilon_1} & w=\varepsilon_1 \\
            \frac{2\varepsilon_2+\frac{3}{2}\varepsilon_2^2}{\varepsilon_2} & w=\varepsilon_2 \\
            \star & \star
        \end{piecewise} \\
        &= 1+w\begin{piecewise}
            2 & w=\varepsilon_1 \\
            2+\frac{3}{2}\varepsilon_2 & w=\varepsilon_2 \\
            \star & \star
        \end{piecewise} \\
        &= 1+w\paren*{2+(w-\varepsilon_1)\begin{piecewise}
            \frac{\frac{3}{2}\varepsilon_2}{\varepsilon_2-\varepsilon_1} & w=\varepsilon_2 \\
            \star & \star
        \end{piecewise}}
    \end{align*}

    We now have a problematic expression:
    $$
        \frac{\varepsilon_2}{\varepsilon_2-\varepsilon_1}
    $$

    If we attempt to find some extended dual number for which this can be simplified, we won't find a solution. With this being said, if we multiply numerator and denominator by $\varepsilon_2+\varepsilon_1$ we can `set' the expression to be $1$.

    Finally, we have that:
    $$
        F(w) = 1+w\paren*{2+\frac{3}{2}(w-\varepsilon_1)}
    $$

    Regard that when we evaluate this function at each of our points ($0,\varepsilon_1,\varepsilon_2$), we get the correct output. If we expand into `real' and `non-real' parts (keeping in mind that $w$ is not strictly real; see the remark about taking the real part), we should have our solution.
    $$
        F(w) = 1+2w+\frac{3}{2}w^2-\frac{3}{2}\varepsilon_1w \implies f(x)=1+2x+\frac{3}{2}x^2
    $$

    Furthermore, regard that the real part of this function corresponds exactly to the Taylor series at $0$. This is because we've chosen not only a polynomial, but also to interpolate at a single point.
\end{example}

\begin{example}
    As an exercise, let us verify that the minimal polynomial corresponding to $f(0)=2$, $f'(0)=3$ and $f(1)=6$ is given by $f(x)=x^2+3x+2$.

    From $f(0)=2$ and $f'(0)=3$ we have $F(\varepsilon_1)=2+3\varepsilon_1$. Therefore:
    \begin{align*}
        F(w) &= \begin{piecewise}
            2 & w=0 \\
            6 & w=1 \\
            2+3\varepsilon_1 & w=\varepsilon_1 \\
            \star & \star
        \end{piecewise} \\
        &= 2+w\begin{piecewise}
            4 & w=1 \\
            3\frac{\varepsilon_1}{\varepsilon_1} & w=\varepsilon_1 \\
            \star & \star
        \end{piecewise} \\
        &= 2+w\paren*{4+(w-1)\begin{piecewise}
            \frac{-1}{\varepsilon_1-1} & w=\varepsilon_1 \\
            \star & \star
        \end{piecewise}} \\
        &= 2+w\paren*{4+(w-1)\begin{piecewise}
            \varepsilon+1 & w=\varepsilon_1 \\
            \star & \star
        \end{piecewise}} \\
        &= 2+w\paren*{4+(w-1)(\varepsilon_1+1)}
    \end{align*}

    Expanding this out and separating real and non-real parts, we get:
    $$
        F(w)=2+4w+w(w-1)+w(w-1)\varepsilon_1 \implies f(x)=x^2+3x+2
    $$
\end{example}

\begin{theorem}
    Division by an extended dual number can be given by the following, for non-zero real part:
    $$
        \frac{1}{w}=\frac{1}{\Re{w}^{N+1}}\sum_{n=0}^{N}{\paren*{-1}^n\Re{w}^{N-n}(w-\Re{w})^n}
    $$

    Where $N=\deg{w}$. This can be derived from the Taylor series of $\frac{1}{x}$. Furthermore, division by a nilpotent, extended dual number isn't well-defined, as previously mentioned.

    However, let us note that multiplying both sides of this equation by $w$ and $\Re{w}^{N+1}$ gives us the following:

    $$
        \Re{w}^{N+1}=w\cdot\sum_{n=0}^{N}{\paren*{-1}^n\Re{w}^{N-n}(w-\Re{w})^n}
    $$

    This, at least in form, resembles the norm of some complex number, namely that for some $z\in\mathbb{C}$ we have $z\overline{z}=\norm{z}^2$; instead, we have that $w\overline{w}=\norm{w}^{N+1}$. We can define $\overline{w}$, therefore, to be:
    $$
        \overline{w}=\frac{\Re{w}^{N+1}}{w}
    $$

    Even when $N$ is odd, this still isn't really a \textit{norm} since we fail $\norm{w}=0\iff w=0$, despite the fact that:
    $$
        \abs{\Re{w}}=\sqrt[N+1]{w\overline{w}}
    $$

    So really, this is a seminorm. For even $N$ then there's definitely no hope of it being a seminorm; $\sqrt[2n]{x^{2n}}=\abs{x}$ for integer $n$; for odd powers we have $\sqrt[2n+1]{x^{2n+1}}=x$, which is not non-negative\footnotemark.

    \footnotetext{This phrase is problematic in English, see \href{https://ell.stackexchange.com/questions/11543/negation-of-must}{this English Stack Exchange thread}; what we mean to say is that $x$ is not restricted to being non-negative, as a property.}
\end{theorem}

\newpage